{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Churn - Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem Overview\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    " \n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    "\n",
    " \n",
    "\n",
    "To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    " \n",
    "\n",
    "In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_data = pd.read_csv(\"telecom_churn_data.csv\")\n",
    "telecom_churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter High Value Customers\n",
    "\n",
    "In the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n",
    "\n",
    " \n",
    "As mentioned above, you need to predict churn only for the high-value customers. Define high-value customers as follows: **Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).**\n",
    "\n",
    " \n",
    "\n",
    "After filtering the high-value customers, you should get about 29.9k rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_data['total_rech_data_amt_6'] = telecom_churn_data['av_rech_amt_data_6'] * telecom_churn_data['total_rech_data_6']\n",
    "telecom_churn_data['total_rech_data_amt_7'] = telecom_churn_data['av_rech_amt_data_7'] * telecom_churn_data['total_rech_data_7']\n",
    "\n",
    "# drop columns av_rech_amt_data_x,total_rech_data_x (x = 6/7/8)\n",
    "telecom_churn_data.drop(['total_rech_data_6','total_rech_data_7','total_rech_data_8','total_rech_data_9',\n",
    "'av_rech_amt_data_6','av_rech_amt_data_7','av_rech_amt_data_8','av_rech_amt_data_9'],axis = 1,inplace = True)\n",
    "\n",
    "# Avg recharge done = total amount spend would be the sum of total data recharge done & total call/sms recharges\n",
    "telecom_av_rech_6n7 = (telecom_churn_data['total_rech_amt_6'].fillna(0) \n",
    "+ telecom_churn_data['total_rech_amt_7'].fillna(0) \n",
    "+ telecom_churn_data['total_rech_data_amt_6'].fillna(0) \n",
    "+ telecom_churn_data['total_rech_data_amt_7'].fillna(0))/2\n",
    "\n",
    "# take 70 percentile of the calculated average amount\n",
    "percentile_70_6n7 = np.percentile(telecom_av_rech_6n7, 70.0)\n",
    "print(\"70 percentile is : \", percentile_70_6n7)\n",
    "\n",
    "# fitler the given data set based on 70th percentile\n",
    "telecom_hv_cust = telecom_churn_data[telecom_av_rech_6n7 >= percentile_70_6n7]\n",
    "\n",
    "print(\"Dimensions of the filtered dataset:\",telecom_hv_cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Churn Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets introduce a new column \"churn\", values would be either 1 (churn) or 0 (non-churn)\n",
    "# we will calculate churn/non-churn based on the usage as mentioned in the problem statement\n",
    "telecom_hv_cust['churn'] = np.where(telecom_hv_cust[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].sum(axis=1) == 0, 1,0)\n",
    "telecom_hv_cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn vs non churn percentage\n",
    "telecom_hv_cust['churn'].value_counts()/len(telecom_hv_cust)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91% of the customers do not churn. This also points us to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the columns with no variance in their values and drop such columns\n",
    "for i in telecom_hv_cust.columns:\n",
    "    if telecom_hv_cust[i].nunique() == 1:\n",
    "        print(\"\\nColumn\",i,\"has no variance and contains only\", telecom_hv_cust[i].nunique(),\"unique value\")\n",
    "        print(\"Dropping the column\",i)\n",
    "        telecom_hv_cust.drop(i,axis=1,inplace = True)\n",
    "\n",
    "print(\"\\nDimension of the updated dataset:\",telecom_hv_cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the columns with no variance in their values and drop such columns\n",
    "for i in telecom_hv_cust.columns:\n",
    "    if telecom_hv_cust[i].nunique() == 1:\n",
    "        print(\"\\nColumn\",i,\"has no variance and contains only\", telecom_hv_cust[i].nunique(),\"unique value\")\n",
    "        print(\"Dropping the column\",i)\n",
    "        telecom_hv_cust.drop(i,axis=1,inplace = True)\n",
    "\n",
    "print(\"\\nDimension of the updated dataset:\",telecom_hv_cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the null values present in the dataset\n",
    "(telecom_hv_cust.isnull().sum() * 100 / len(telecom_hv_cust)).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns with > 30% of missing values except 9th Month's columns\n",
    "cols = telecom_hv_cust.columns\n",
    "telecom_null_perc = telecom_hv_cust.isnull().sum() * 100 / len(telecom_hv_cust)\n",
    "telecom_null_df = pd.DataFrame({'col_name': cols,\n",
    "                                 'perc_null': telecom_null_perc})\n",
    "\n",
    "drop_cols = telecom_null_df.loc[(telecom_null_df[\"col_name\"].str.contains('_9')==False) & (telecom_null_df[\"perc_null\"] > 30.0)][\"col_name\"]\n",
    "print(\"list of columns dropped:\",drop_cols)\n",
    "\n",
    "# lets drop these columns\n",
    "telecom_hv_cust.drop(drop_cols, axis=1,inplace = True)\n",
    "telecom_hv_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check for columns that can be changed to integers, floats or date types\n",
    "object_col_data = telecom_hv_cust.select_dtypes(include=['object'])\n",
    "print(object_col_data.iloc[0])\n",
    "\n",
    "# observation : all the columns below can be converted to date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "for col in object_col_data.columns:\n",
    "    telecom_hv_cust[col] = pd.to_datetime(telecom_hv_cust[col])\n",
    "\n",
    "telecom_hv_cust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = telecom_hv_cust.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove highly correlated features for two reasons - \n",
    "\n",
    "- PCA needs non-correlated features to perform well\n",
    "- Decision Trees are immune to multicollinearity anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - lets check the correlation amongst the features, drop the highly correlated ones\n",
    "\n",
    "We need to find a way to get correlation value list of more than 0.70 and less than 0.70 and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = cols_to_drop.index.to_list()\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_arr = []\n",
    "\n",
    "for i in cols_to_drop:\n",
    "    for j in i:\n",
    "        cols_to_drop_arr.append(j)\n",
    "\n",
    "cols_to_drop_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hv_cust.drop(cols_to_drop_arr, axis=1, inplace=True)\n",
    "telecom_hv_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to drop 9th column\n",
    "\n",
    "cols_to_drop = []\n",
    "\n",
    "for col in telecom_hv_cust.columns:\n",
    "    if '_9' in col:\n",
    "        cols_to_drop.append(col)\n",
    "        \n",
    "telecom_hv_cust.drop(cols_to_drop, axis=1, inplace=True)\n",
    "telecom_hv_cust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do -  \n",
    "\n",
    "- We have to do PCA and Xgboost\n",
    "- We have to check if data is imbalanced, we would then need to do oversampling to balance it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
